# -*- coding: utf-8 -*-
"""udemy .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u__M9HIMXhpW1A7QObG7uMYP5ZKz4f9U
"""

# Cell 1 — Imports & Load CSV
import pandas as pd
import numpy as np
import re
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

DATA_PATH = "udemy_courses.csv"  # update path if needed
df = pd.read_csv(DATA_PATH)
print("Rows, cols:", df.shape)
display(df.head())

# Cell 2 — EDA & Basic Cleaning
# Quick info
print(df.info())
print(df.describe(include='all').T[['top','freq']].head())

# Normalize column names
df.columns = [c.strip() for c in df.columns]

# Parse content_duration (hours) -> numeric
def parse_duration(x):
    try:
        return float(x)
    except:
        # handle empty or malformed
        import re
        m = re.search(r'[\d\.]+', str(x))
        return float(m.group()) if m else np.nan

df['content_duration_hr'] = df['content_duration'].apply(parse_duration)

# Map 'level' to simplified difficulty
def map_level(l):
    l = str(l).lower()
    if 'beginner' in l:
        return 'Beginner'
    if 'intermediate' in l:
        return 'Intermediate'
    if 'all' in l:
        return 'All'
    if 'advanced' in l:
        return 'Advanced'
    return 'Unknown'

df['level_simple'] = df['level'].apply(map_level)

# Basic stats
print("Duration stats (hrs):")
print(df['content_duration_hr'].describe())

# show top subjects
print("Top subjects:")
display(df['subject'].value_counts().head(10))

# Save cleaned sample
df.to_csv("udemy_courses_cleaned.csv", index=False)

# Cell 3 — Text preprocessing & TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+',' ', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = [t for t in text.split() if t not in STOPWORDS]
    return ' '.join(tokens)

# Combine title + subject + level to make corpus
df['text_raw'] = (df['course_title'].fillna('') + ' ' + df['subject'].fillna('') + ' ' + df['level_simple'].fillna(''))
df['clean_text'] = df['text_raw'].apply(clean_text)

# Fit TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
tfidf_mat = tfidf.fit_transform(df['clean_text'].astype(str))
print("TF-IDF matrix shape:", tfidf_mat.shape)

# Cell 4 — Compute transformer embeddings (optional, slow first run)
# Run this locally once and save course_embeddings.npy for fast startup/deploy
!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize

model = SentenceTransformer('paraphrase-MiniLM-L3-v2')  # small & fast
texts = df['clean_text'].astype(str).tolist()
emb = model.encode(texts, show_progress_bar=True)
emb_norm = normalize(np.array(emb), axis=1, norm='l2')
np.save("udemy_course_embeddings.npy", emb_norm.astype('float32'))
print("Saved embeddings shape:", emb_norm.shape)

# Cell 5 — Time matching: continuous score using content_duration_hr
# user_hours_per_week -> match to course total hours and produce score in [0,1]

def time_score_continuous(user_hours_per_week, course_total_hours, expected_weeks=4):
    """
    Estimate required weekly hours = course_total_hours / expected_weeks.
    Score = clamp(user_hours_per_week / required_weekly_hours, 0, 1)
    If course_total_hours missing -> fallback to heuristic based on num_lectures.
    """
    try:
        ch = float(course_total_hours)
        if np.isnan(ch) or ch <= 0:
            raise ValueError
        required_weekly = max(0.5, ch / expected_weeks)  # avoid division by tiny
        score = user_hours_per_week / required_weekly
        return float(max(0.0, min(1.0, score)))
    except:
        # fallback: use num_lectures as proxy (more lectures -> longer)
        try:
            nl = float(df_row.get('num_lectures', np.nan))
            # map 0-200 lectures to 0-1
            return float(min(1.0, nl / 200.0))
        except:
            return 0.5  # neutral

# Example usage:
print("Example time score:", time_score_continuous(4, 10, expected_weeks=4))

# Cell 6 — Similarity & scoring utilities
from sklearn.metrics.pairwise import cosine_similarity

# Load saved transformer embeddings if available for fast use
EMB_PATH = "udemy_course_embeddings.npy"
if Path(EMB_PATH).exists():
    emb_norm = np.load(EMB_PATH)
    print("Loaded transformer embeddings:", emb_norm.shape)
else:
    emb_norm = None
    print("Transformer embeddings not found. Using TF-IDF only for semantic similarity.")

def semantic_sim_by_index(idx):
    if emb_norm is not None:
        sims = emb_norm.dot(emb_norm[idx])
        sims = np.clip(sims, 0, 1)
        return sims
    else:
        # TF-IDF fallback
        v = tfidf_mat[idx]
        sims = cosine_similarity(v, tfidf_mat).ravel()
        # normalize to 0-1
        mn, mx = sims.min(), sims.max()
        sims_norm = (sims - mn) / (mx - mn + 1e-9)
        return sims_norm

def semantic_sim_by_query_text(query, use_transformer=False):
    q = clean_text(query)
    if use_transformer and emb_norm is not None:
        q_emb = model.encode([q])
        q_emb_n = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)
        sims = emb_norm.dot(q_emb_n[0])
        return np.clip(sims, 0, 1)
    else:
        qv = tfidf.transform([q])
        sims = cosine_similarity(qv, tfidf_mat).ravel()
        mn, mx = sims.min(), sims.max()
        sims_norm = (sims - mn) / (mx - mn + 1e-9)
        return sims_norm

def skill_score(user_skill, course_level):
    # map levels to numeric
    map_v = {'Beginner':0, 'All':0, 'Intermediate':1, 'Advanced':2, 'Unknown':1}
    try:
        us = {'Beginner':0,'Intermediate':1,'Advanced':2}.get(user_skill,1)
        cs = map_v.get(course_level,1)
        dist = abs(us - cs)
        if dist == 0: return 1.0
        if dist == 1: return 0.6
        return 0.2
    except:
        return 0.6

def combine_scores(sem_sim_vec, context_scores, alpha=0.7):
    # sem_sim_vec and context_scores are numpy arrays aligned with df
    final = alpha * sem_sim_vec + (1-alpha) * context_scores
    return final

# Cell 7 — Demo recommendation call (non-streamlit)
def recommend_by_query_context(query, user_skill='Beginner', user_hours_per_week=4.0, alpha=0.7, topk=10, use_transformer=False):
    sem = semantic_sim_by_query_text(query, use_transformer=use_transformer)
    # compute context score per course: combine time_score and skill_score (weighted)
    time_scores = np.zeros(len(df))
    skill_scores = np.zeros(len(df))
    for i, row in df.iterrows():
        # time score: use content_duration_hr
        time_scores[i] = time_score_continuous(user_hours_per_week, row.get('content_duration_hr', np.nan), expected_weeks=4)
        skill_scores[i] = skill_score(user_skill, row.get('level_simple', 'Unknown'))
    # context combined: weight skill heavier
    context_scores = 0.6 * skill_scores + 0.4 * time_scores
    final = combine_scores(sem, context_scores, alpha=alpha)
    idxs = np.argsort(-final)[:topk]
    results = []
    for idx in idxs:
        results.append({
            'index': int(idx),
            'title': df.loc[idx,'course_title'],
            'subject': df.loc[idx,'subject'],
            'duration_hr': df.loc[idx,'content_duration_hr'],
            'level': df.loc[idx,'level_simple'],
            'score': float(final[idx]),
            'semantic': float(sem[idx]),
            'context': float(context_scores[idx])
        })
    return results

# Try it
res = recommend_by_query_context("beginner python data analysis", user_skill='Beginner', user_hours_per_week=3.0, topk=5, use_transformer=False)
for r in res:
    print(r)

